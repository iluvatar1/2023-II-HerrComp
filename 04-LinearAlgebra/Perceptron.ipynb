{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bdb43a8-bbf2-4f7d-9ad4-c42fdd92de00",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba1c46c-bbc6-446d-b610-8095fc85ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"NONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb7470-b6d2-4156-abda-fd450e28b26a",
   "metadata": {},
   "source": [
    "# Simple neural network: A perceptron\n",
    "Based on https://www.youtube.com/watch?v=kft1AJ9WVDk\n",
    "\n",
    "This is what we want to train our neural network with:\n",
    "\n",
    "![Alt Text](fig/neuralnetwork/inputs.png)\n",
    "\n",
    "And we want to predict the new output (try to guess the rule)\n",
    "\n",
    "![Alt Text](fig/neuralnetwork/newoutput.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe5ed5e-2305-4836-b041-1c09d4fea756",
   "metadata": {},
   "source": [
    "This is the neural network that we are going to use (you can also use http://alexlenail.me/NN-SVG/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04813a6c-58da-4431-b5f7-36334efa053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnv import NNV\n",
    "\n",
    "layersList = [\n",
    "    {\"title\":\"input\", \"units\": 3, \"color\": \"darkBlue\"},\n",
    "    {\"title\":\"hidden 1\\n(sigmoid)\", \"units\": 1, \"edges_color\":\"red\", \"edges_width\":2},\n",
    "    {\"title\":\"output\\n(sigmoid)\", \"units\": 1,\"color\": \"darkBlue\"},\n",
    "]\n",
    "\n",
    "NNV(layersList).render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f7df3d-42e5-4bb5-84e3-eb70afdcf97e",
   "metadata": {},
   "source": [
    "To understand better the training, let's show explicitly the weights\n",
    "![weightds](fig/neuralnetwork/weights.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f7bb4-3ce4-41c3-8e7d-4a6df86667e7",
   "metadata": {},
   "source": [
    "Here $\\phi$ is called the activation function, and there are several proposals to it. We will use a sigmoid function\n",
    "$$\n",
    "f(x) = \\dfrac{1}{1+\\exp(-x)},\n",
    "$$\n",
    "where $x = \\sum x_i w_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf6c30d-f219-41d5-a835-42f3ed72c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_context('poster')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def sigmoid(x) :\n",
    "    return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "xdata = np.linspace(-6.0, 6.0, 100)\n",
    "plt.plot(xdata, sigmoid(xdata))\n",
    "# Highlight x=0 and y=0 axes\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=2.5)  # Horizontal line at y=0\n",
    "plt.axvline(0, color='black', linestyle='--', linewidth=2.5)  # Vertical line at x=0\n",
    "# Add labels to the x-axis and y-axis\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(rf\"sigmoid(x)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6feb701-24f9-4335-95d1-299cf00fe3cd",
   "metadata": {},
   "source": [
    "# Basic Implementation\n",
    "For this very basic nn, we will:\n",
    "- set the input or start of the algorithm:\n",
    "    + Random weights $w_i$\n",
    "    + Set the training inputs and outputs\n",
    "- Create an iteration function to perform the training for `nsteps` (initially 1)\n",
    "\n",
    "The we just iterate once and check what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa38ab3-b4f2-4f4e-abd1-f849cba2e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x) :\n",
    "    return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "def get_training_inputs():\n",
    "    return np.array([[0, 0, 1],\n",
    "                     [1, 1, 1], \n",
    "                     [1, 0, 1],\n",
    "                     [0, 1, 1]])\n",
    "\n",
    "def get_training_outputs():\n",
    "    return np.array([0, 1, 1, 0]).reshape(4, 1)\n",
    "\n",
    "def get_init_weights():\n",
    "    \"\"\"\n",
    "    Initially, simply return random weights in [-1, 1)\n",
    "    \"\"\"\n",
    "    return np.random.uniform(-1.0, 1.0, size=(3, 1))\n",
    "\n",
    "def training_one_step(training_inputs, training_outputs, initial_weights):\n",
    "    # iter only once\n",
    "    input_layer = training_inputs\n",
    "    outputs = sigmoid(np.dot(input_layer, initial_weights))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3ae1b-da69-4eb4-aa96-f2c847d682d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) # what happens if you comment this?\n",
    "inputs_t = get_training_inputs()\n",
    "outputs_t = get_training_outputs()\n",
    "weights = get_init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a4dd5e-2714-4a77-85f8-943a7d4ea3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = training_one_step(inputs_t, outputs_t, weights)\n",
    "print(\"Training outputs:\")\n",
    "print(outputs_t)\n",
    "print(\"Results after one step training:\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1088fc-abd7-443b-9c58-1bf7a73a374f",
   "metadata": {},
   "source": [
    "# Improving the training\n",
    "These results are not optimal, and depend a lot on the initial weights. Also, we are not yet comparing with the expecting output for the training data. We are now going to include it and add correction terms to the weights, so we will be using back-propagation. Our algorithm is now:\n",
    "- Take each input from the training data.\n",
    "- Compute the error, i.e. the difference between the output and the expected one, `output - expectedoutput`. \n",
    "- According to the error, adjust the weights\n",
    "- Repeat this many times, hopefully getting convergence , and also being able to apply our nn to new cases not used already.\n",
    "\n",
    "But how to adjust the weights? There are several techniques based on the actual error $\\Delta$. Here we will use error weighted derivative. Given the form of the sigmoid function, this increases the adjust if the derivative is larger, and viceversa. It can be expressed as \n",
    "\n",
    "$$\n",
    "\\Delta w = \\Delta \\times \\text{input} \\times \\phi'(output),\n",
    "$$\n",
    "where $\\phi'$ is the derivative of the activation function. In our one-dimensional case we can compute it easily, but with more complex problems it becomes a gradient and its efficient computation is very important (remember automatic differentiation?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717a1c8e-348e-430c-8d66-9ca56233bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "def train_nn(training_inputs, training_outputs, initial_weights, niter, errors_data):\n",
    "    w = initial_weights\n",
    "    for ii in range(niter):\n",
    "        # Forward propagation\n",
    "        input_layer = training_inputs\n",
    "        outputs = sigmoid(np.dot(input_layer, w))\n",
    "        # Backward propagation\n",
    "        errors = training_outputs - outputs\n",
    "        deltaw = errors*sigmoid_prime(outputs)\n",
    "        deltaw = np.dot(input_layer.T, deltaw)\n",
    "        w += deltaw\n",
    "        # Save errors for plotting later\n",
    "        errors_data[ii] = errors.reshape((4,))\n",
    "    return outputs, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe687ae-0949-4b98-bc98-d2678bcda87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) # what happens if you comment this?\n",
    "inputs_t = get_training_inputs()\n",
    "outputs_t = get_training_outputs()\n",
    "weights = get_init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff3a2c-be33-4a6d-8a06-684105e5498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NITER = 5000\n",
    "errors = np.zeros((NITER, 4))\n",
    "outputs, weights = train_nn(inputs_t, outputs_t, weights, NITER, errors)\n",
    "print(\"Training outputs:\")\n",
    "print(outputs_t)\n",
    "print(\"Results after training:\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782708c7-59a7-43d3-810d-f2e1ad09c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "ax[0].plot(range(NITER), errors)\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Errors\")\n",
    "ax[1].loglog(range(NITER), np.abs(errors))\n",
    "ax[1].set_xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ddd526-d79d-456f-a3a7-ec7532ef21b9",
   "metadata": {},
   "source": [
    "It seems that our network is very well trained, But how does it perform with a new input? let's check with `[1, 0, 0]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d307d3-dd66-4b6c-abdf-8a16818fbcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(weights)\n",
    "#print(weights.shape)\n",
    "input_new = np.array([1, 0, 0]).reshape(3, 1)\n",
    "#print(input_new)\n",
    "#print(input_new.shape)\n",
    "#print(np.sum(weights*input_new))\n",
    "print(sigmoid(np.sum(weights*input_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8601db1c-21d5-47a3-86da-ea602f23ffac",
   "metadata": {},
   "source": [
    "Which is basically one, as expected.\n",
    "There are more topics related to this that we have not used, like more layers, more neurons per hidden layer, bias on the activation function, and a lot of other details, but hopefully you now see how a neural network works on the core.\n",
    "\n",
    "Recommended lectures:\n",
    "- 3blue1brown Neural Networks: https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n",
    "- Neiral networks from scratch: https://www.youtube.com/watch?v=9RN2Wr8xvro\n",
    "\n",
    "TODO:\n",
    "- Add a second layer and compare the convergence\n",
    "- Add an example using pythorch/tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc5accc-697f-4eaa-9285-81f833a02d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
